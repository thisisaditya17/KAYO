{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Proposition-Based Chunking ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crazygynkz/Desktop/KAYO/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/crazygynkz/Desktop/KAYO/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"#### Proposition-Based Chunking ####\")\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain import hub\n",
    "import textract\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = \"AIzaSyADAsholuvCPecxj8W9zj-TkZ431vtSMTc\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash', google_api_key=api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, you are walking through a prehistoric forest, you hear a twig snap behind you. You turn, slowly, hoping that it isn't what you think it is. Standing behind you is a Stegosaurus, chomping away on some leafy ferns. You breathe a sigh of relief. You were afraid it was a predator with its eye on you as a tasty snack. Of course, that was millions of years ago, so it could only happen in your imagination. In fact it was so long ago that there were no people around at all; there weren't even any apes or monkeys yet!\n",
      "\n",
      "The prehistoric world of the dinosaurs is both strange and exciting. Many things were very different from what you see today. Some dinosaurs stood taller than buildings, and others weighed as much as your entire family put together. Some dinosaurs were small enough to fit in your backpack. Some would not hesitate to eat you on the spot if they were hungry. Others spent all day chomping on ferns and other plants. The one kind of dinosaur that never existed was a dull dinosaur!\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text = textract.process(\"test.txt\")\n",
    "text = text.decode('utf-8')\n",
    "print(text)\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0\n",
      "Done with 1\n",
      "You have 20 propositions\n"
     ]
    }
   ],
   "source": [
    "obj = hub.pull(\"wfh/proposal-indexing\")\n",
    "\n",
    "\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Sentences)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that extracts key propositions from text.\"),\n",
    "    (\"human\", \"Extract key propositions from the following text:\\n\\n{text}\"),\n",
    "    (\"human\", \"Format your response as a JSON object with a 'sentences' key containing a list of proposition strings.\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, output_parser=parser)\n",
    "\n",
    "def get_propositions(text):\n",
    "    try:\n",
    "        result = chain.run(text=text)\n",
    "        return result.sentences\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extraction: {e}\")\n",
    "        return []\n",
    "\n",
    "paragraphs = text.split(\"\\n\\n\")\n",
    "text_propositions = []\n",
    "\n",
    "for i, para in enumerate(paragraphs):\n",
    "    propositions = get_propositions(para)\n",
    "    text_propositions.extend(propositions)\n",
    "    print(f\"Done with {i}\")\n",
    "    time.sleep(8)   \n",
    "\n",
    "print(f\"You have {len(text_propositions)} propositions\")\n",
    "#print(text_propositions)\n",
    "#print(len(text_propositions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crazygynkz/Desktop/KAYO/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 1/1 [00:45<00:00, 45.68s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('Snowflake/snowflake-arctic-embed-l')\n",
    "\n",
    "\n",
    "embeddings = embedding_model.encode(text_propositions, show_progress_bar=True)\n",
    "\n",
    "embeddings_np = numpy.array(embeddings).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss #facebook ai similarity search \n",
    "\n",
    "query = \"safasf\"\n",
    "\n",
    "\n",
    "def retrieve(query, nearest_neighbours=5):\n",
    "\n",
    "    dimension = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "\n",
    "    query_embedding = embedding_model.encode(query, show_progress_bar=True)\n",
    "    query_embedding_np = numpy.array([query_embedding]).astype('float32')\n",
    "    distance, indices = index.search(query_embedding_np, nearest_neighbours)\n",
    "\n",
    "    contexts = []\n",
    "    for i in range(len(indices[0])):\n",
    "        chunk_index = indices[0][i]\n",
    "        similarity = 1 / (1 + distance[0][i])\n",
    "        chunk_text = text_propositions[chunk_index]\n",
    "        contexts.append(f\"Rank {i+1}: {chunk_text} | Similarity: {similarity:.4f}\")\n",
    "\n",
    "    return \"\\n\\n\".join(contexts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:13<00:00, 13.86s/it]\n"
     ]
    }
   ],
   "source": [
    "retrieved_context = retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the following question based on the provided context:\n",
      "\n",
      "Question: safasf\n",
      "\n",
      "Context: Rank 1: You hope it isn't a predator. | Similarity: 0.6786\n",
      "\n",
      "Rank 2: The event happened millions of years ago. | Similarity: 0.6724\n",
      "\n",
      "Rank 3: There were no people around millions of years ago. | Similarity: 0.6665\n",
      "\n",
      "Rank 4: Many things were very different from what you see today. | Similarity: 0.6662\n",
      "\n",
      "Rank 5: There were no apes or monkeys millions of years ago. | Similarity: 0.6652\n",
      "\n",
      "Provide relevant answers to the question based on the context.\n",
      "Don’t justify your answers.\n",
      "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
      "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Provide relevant answers to the question based on the context.\n",
    "Don’t justify your answers.\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
    "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "\"\"\")\n",
    "\n",
    "prompt = prompt_template.format(context=retrieved_context, question=query)\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot answer the question. The provided context does not contain any relevant information to answer \"safasf\". \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "api_key = \"AIzaSyADAsholuvCPecxj8W9zj-TkZ431vtSMTc\"\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found in environment variables.\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "llm = genai.GenerativeModel('gemini-1.5-pro')\n",
    "\n",
    "response = llm.generate_content(prompt)\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain the answer to the question \"Relations between India and Singapore\". \n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_prompt_template = ChatPromptTemplate.from_template(\"\"\"The original query is as follows: {query}\n",
    "We have provided an existing answer: {existing_answer}\n",
    "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
    "Don't mention Refined Answer\n",
    "\"\"\")\n",
    "\n",
    "refined_prompt = refined_prompt_template.format(query=query, existing_answer=response.text, context=retrieved_context)\n",
    "\n",
    "response = llm.generate_content(refined_prompt)\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
